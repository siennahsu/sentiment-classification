# -*- coding: utf-8 -*-
"""animal-facts-clfn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t692gdRsYsskSGmf-A95GY7YJjjANjIB
"""

# importing all relevant libraries
import nltk
nltk.download('wordnet')
nltk.download('punkt')
import pandas as pd
import random
random.seed(550)
import copy

"""# Preprocessing"""

# opening the fact and fake files
with open("/content/facts.txt") as f:
    fact_corpus = f.read()

with open("/content/fakes.txt") as f:
    fake_corpus = f.read()

# attach the label to the corpus, shuffle it

corpus_list = _

fact_list = fact_corpus.split("\n")
for i in range(len(fact_list)):
  fact_list[i] = (fact_list[i], 1)  # 1 means fact

fake_list = fake_corpus.split("\n")
for i in range(len(fake_list)):
  fake_list[i] = (fake_list[i], 0)  # 0 means fake

corpus_list = fact_list + fake_list

random.Random(550).shuffle(corpus_list)

# split the corpus into train and test sets
limiter = int(len(corpus_list) * 0.8)

X_train = []
y_train = []
for i in range(limiter):
  X_train.append(corpus_list[i][0])
  y_train.append(corpus_list[i][1])

X_test = []
y_test = []
for i in range(limiter, len(corpus_list)):
  X_test.append(corpus_list[i][0])
  y_test.append(corpus_list[i][1])

"""## Lemmatization"""

# tokenize the words in each datapoint and then lemmatize the words
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer

wordnet_lemmatizer = WordNetLemmatizer()
X_train_le = copy.deepcopy(X_train)
X_test_le = copy.deepcopy(X_test)

for i in range(len(X_train_le)):
  X_train_le[i] = X_train_le[i].lower()
  X_train_le[i]= X_train_le[i].replace(",", "").replace(".", "").replace("'", "").replace("-", " ")
  X_train_le[i] = word_tokenize(X_train_le[i])

  for j in range(len(X_train_le[i])):
    X_train_le[i][j] = wordnet_lemmatizer.lemmatize(X_train_le[i][j])

  X_train_le[i] = " ".join(X_train_le[i])

# count-vectorize the words
from sklearn.feature_extraction.text import CountVectorizer

# Create a Vectorizer Object
vectorizer_le = CountVectorizer(ngram_range = (1, 1), stop_words='english')
vectorizer_le.fit(X_train_le)

# Encode X_train_le
X_train_le = vectorizer_le.transform(X_train_le).toarray()

# Define a function to streamline lemmatization and vectorization
def lemma_preprocess(list_of_sen):
  for i in range(len(list_of_sen)):
    list_of_sen[i] = list_of_sen[i].lower()
    list_of_sen[i]= list_of_sen[i].replace(",", "").replace(".", "").replace("'", "").replace("-", " ")
    list_of_sen[i] = word_tokenize(list_of_sen[i])

    for j in range(len(list_of_sen[i])):
      list_of_sen[i][j] = wordnet_lemmatizer.lemmatize(list_of_sen[i][j])

    list_of_sen[i] = " ".join(list_of_sen[i])

  return vectorizer_le.transform(list_of_sen).toarray()

# Lemmatize and vectorize the test data
X_test_le = lemma_preprocess(X_test_le)

"""## Unigram"""

X_train_uni = copy.deepcopy(X_train)
X_test_uni = copy.deepcopy(X_test)

for i in range(len(X_train_uni)):
  X_train_uni[i] = X_train_uni[i].lower()
  X_train_uni[i]= X_train_uni[i].replace(",", "").replace(".", "").replace("'", "").replace("-", " ")

# count-vectorize the words
# Create a Vectorizer Object
vectorizer_uni = CountVectorizer(ngram_range = (1, 1), stop_words='english')
vectorizer_uni.fit(X_train_uni)

# Encode X_train_le
X_train_uni = vectorizer_uni.transform(X_train_uni).toarray()

# Define a function to streamline vectorization
def uni_preprocess(list_of_sen):
  for i in range(len(list_of_sen)):
    list_of_sen[i] = list_of_sen[i].lower()
    list_of_sen[i]= list_of_sen[i].replace(",", "").replace(".", "").replace("'", "").replace("-", " ")

  return vectorizer_uni.transform(list_of_sen).toarray()

# Vectorize the test data
X_test_uni = uni_preprocess(X_test_uni)

"""## Bigram"""

X_train_bi = copy.deepcopy(X_train)
X_test_bi = copy.deepcopy(X_test)

for i in range(len(X_train_bi)):
  X_train_bi[i] = X_train_bi[i].lower()
  X_train_bi[i]= X_train_bi[i].replace(",", "").replace(".", "").replace("'", "").replace("-", " ")

# count-vectorize the words
# Create a Vectorizer Object
vectorizer_bi = CountVectorizer(ngram_range = (2, 2), stop_words='english')
vectorizer_bi.fit(X_train_bi)

# Encode X_train_le
X_train_bi = vectorizer_bi.transform(X_train_bi).toarray()

# Define a function to streamline vectorization
def bi_preprocess(list_of_sen):
  for i in range(len(list_of_sen)):
    list_of_sen[i] = list_of_sen[i].lower()
    list_of_sen[i]= list_of_sen[i].replace(",", "").replace(".", "").replace("'", "").replace("-", " ")

  return vectorizer_bi.transform(list_of_sen).toarray()

# Vectorize the test data
X_test_bi = bi_preprocess(X_test_bi)

"""# Logistic regression

## Unigram
"""

# Tune the hyperparameters penalty, C, max_iter
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

param_grid = {'C': [0.1, 1, 10, 100, 1000],
              'penalty': ['l1', 'l2'],
              'max_iter': [1, 2, 3, 4, 5, 10, 15, 50]}

lr_uni_model = GridSearchCV(LogisticRegression(solver='liblinear'), param_grid, refit=True, verbose=3, n_jobs=-1)
lr_uni_model.fit(X_train_uni, y_train)

print("Logistic Regression Model with Unigram Proprocessing - Best Hyperparameters\n")
print(lr_uni_model.best_params_)
print()

lr_uni_model_predictions = lr_uni_model.predict(X_test_uni)
lr_uni_report = classification_report(y_test, lr_uni_model_predictions)

print("Logistic Regression Model with Unigram Proprocessing - Performance Report\n")
print(lr_uni_report)
print()

test_predict = "Cats are known to play secret hitler every sunday"
to_test = uni_preprocess([test_predict])
lr_uni_model.predict(to_test)

"""## Lemmatization"""

lr_le_model = GridSearchCV(LogisticRegression(solver='liblinear'), param_grid, refit=True, verbose=3, n_jobs=-1)
lr_le_model.fit(X_train_le, y_train)

print("Logistic Regression Model with Lemmatization Proprocessing - Best Hyperparameters\n")
print(lr_le_model.best_params_)
print()

lr_le_model_predictions = lr_le_model.predict(X_test_le)
lr_le_report = classification_report(y_test, lr_le_model_predictions)

print("Logistic Regression Model with Lemmatization Proprocessing - Performance Report\n")
print(lr_le_report)
print()

"""## Bigram"""

lr_bi_model = GridSearchCV(LogisticRegression(solver='liblinear'), param_grid, refit=True, verbose=3, n_jobs=-1)
lr_bi_model.fit(X_train_bi, y_train)

print("Logistic Regression Model with Bigram Proprocessing - Best Hyperparameters\n")
print(lr_bi_model.best_params_)
print()

lr_bi_model_predictions = lr_bi_model.predict(X_test_bi)
lr_bi_report = classification_report(y_test, lr_bi_model_predictions)

print("Logistic Regression Model with Bigram Proprocessing - Performance Report\n")
print(lr_bi_report)
print()

"""# SVM

## Unigram
"""

# Tune the hyperparameters penalty, C, max_iter
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix

param_grid = {'C': [0.1, 1, 10, 100, 1000],
              'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
              'degree' : [2, 3, 4] ,
              'gamma' : ['scale', 'auto']}

svm_uni_model = GridSearchCV(SVC(), param_grid, refit=True, verbose=3, n_jobs=-1)
svm_uni_model.fit(X_train_uni, y_train)

print("SVM Model with Unigram Proprocessing - Best Hyperparameters\n")
print(svm_uni_model.best_params_)
print()

svm_uni_model_predictions = svm_uni_model.predict(X_test_uni)
svm_uni_report = classification_report(y_test, svm_uni_model_predictions)

print("SVM Model with Unigram Proprocessing - Performance Report\n")
print(svm_uni_report)
print()

test_predict = "Cats are known to play secret hitler every sunday"
to_test = uni_preprocess([test_predict])
svm_uni_model.predict(to_test)

"""## Lemmatization"""

svm_le_model = GridSearchCV(SVC(), param_grid, refit=True, verbose=3, n_jobs=-1)
svm_le_model.fit(X_train_le, y_train)

print("SVM Model with Lemmatization Proprocessing - Best Hyperparameters\n")
print(svm_le_model.best_params_)
print()

svm_le_model_predictions = svm_le_model.predict(X_test_le)
svm_le_report = classification_report(y_test, svm_le_model_predictions)

print("SVM Model with Lemmatization Proprocessing - Performance Report\n")
print(svm_le_report)
print()

"""## Bigram"""

svm_bi_model = GridSearchCV(SVC(), param_grid, refit=True, verbose=3, n_jobs=-1)
svm_bi_model.fit(X_train_bi, y_train)

print("SVM Model with Bigram Proprocessing - Best Hyperparameters\n")
print(svm_bi_model.best_params_)
print()

svm_bi_model_predictions = svm_bi_model.predict(X_test_bi)
svm_bi_report = classification_report(y_test, svm_bi_model_predictions)

print("SVM Model with Bigram Proprocessing - Performance Report\n")
print(svm_bi_report)
print()

"""# Naive Bayes

## Unigram
"""

# Tune the hyperparameters penalty, C, max_iter
from sklearn.model_selection import GridSearchCV
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix

param_grid = {'alpha': [0, 1e-12, 1e-10, 1e-08] ,
              'class_prior': [[0.5, 0.5], None]}

mnb_uni_model = GridSearchCV(MultinomialNB(force_alpha=True), param_grid, refit=True, verbose=3, n_jobs=-1)
mnb_uni_model.fit(X_train_uni, y_train)

print("Multinomial Naive Bayes Model with Unigram Proprocessing - Best Hyperparameters\n")
print(mnb_uni_model.best_params_)
print()

mnb_uni_model_predictions = mnb_uni_model.predict(X_test_uni)
mnb_uni_report = classification_report(y_test, mnb_uni_model_predictions)

print("Multinomial Naive Bayes Model with Unigram Proprocessing - Performance Report\n")
print(mnb_uni_report)
print()

test_predict = "Cats are known to play secret hitler every sunday"
to_test = uni_preprocess([test_predict])
mnb_uni_model.predict(to_test)

"""## Lemmatization"""

mnb_le_model = GridSearchCV(MultinomialNB(force_alpha=True), param_grid, refit=True, verbose=3, n_jobs=-1)
mnb_le_model.fit(X_train_le, y_train)

print("Multinomial Naive Bayes Model with Lemmatization Proprocessing - Best Hyperparameters\n")
print(mnb_le_model.best_params_)
print()

mnb_le_model_predictions = mnb_le_model.predict(X_test_le)
mnb_le_report = classification_report(y_test, mnb_le_model_predictions)

print("Multinomial Naive Bayes Model with Lemmatization Proprocessing - Performance Report\n")
print(mnb_le_report)
print()

"""## Bigram"""

mnb_bi_model = GridSearchCV(MultinomialNB(force_alpha=True), param_grid, refit=True, verbose=3, n_jobs=-1)
mnb_bi_model.fit(X_train_bi, y_train)

print("Multinomial Naive Bayes Model with Bigram Proprocessing - Best Hyperparameters\n")
print(mnb_bi_model.best_params_)
print()

mnb_bi_model_predictions = mnb_bi_model.predict(X_test_bi)
mnb_bi_report = classification_report(y_test, mnb_bi_model_predictions)

print("Multinomial Naive Bayes Model with Bigram Proprocessing - Performance Report\n")
print(mnb_bi_report)
print()

"""# Model comparison"""

models = [(93,"LR-Uni"), (93,"LR-Lemma"), (68,"LR-Bi"), (89,"SVM-Uni"), (90,"SVM-Lemma"),
          (54,"SVM-Bi"),(93,"MNB-Uni"), (95, "MNB-Lemma"), (74, "MNB-Bi")]

models.sort(reverse=True)

models

x = []
y = []
for model in models:
  x.append(model[0])
  y.append(model[1])

import seaborn as sns
import matplotlib.pyplot as plt
sns.set_theme(style="whitegrid")


# Initialize the matplotlib figure
f, ax = plt.subplots(figsize=(8, 4))

# Plot the total crashes
sns.set_color_codes("pastel")
sns.barplot(x=x, y=y, color="b")

# Add a legend and informative axis label
ax.set(xlim=(50, 100), ylabel="Model",
       xlabel="Accuracy (%)")
ax.bar_label(ax.containers[0])
sns.despine(left=True, bottom=True)